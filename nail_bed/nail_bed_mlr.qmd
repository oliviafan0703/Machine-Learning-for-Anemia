---
title: "Multivariate Linear Regression"
subtitle: "Nail Bed Images"
date: "Nov 14, 2022"
format: pdf
editor: visual
bibliography: references.bib
link-citations: true
output:
  pdf_document: default
---

## 1. Introduction

Anemia is as a life-threatening disease that affects 2 billion, or approximately 1 in 3 people world wide [@GTechdissertation]. Patients suffering from chronic anemia require frequent monitoring of indicators such as the Hgb concentration in blood to track the progression of their disease. Despite high prevalence, however, the current diagnostic process requires blood tests which causes discomfort and trauma in patients, in addition to incurring high monetary cost, especially in regions of lower socio-economic development [@monetary]. Therefore, we aim to create machine learning models that enable non-invasive inexpensive diagnosis using patients' nail bed images to predict their Hgb concentration.

Previous work focuses on predicting Hgb concentration on three regressions of interest: the fingernail beds, the conjunctiva and the palmar creases. We aim to extend the previous research from several perspectives: The previous study found no significant correlation between blue pixel intensity and gold standard measured Hgb, which we aim to investigate further using more complex machine learning algorithms such as random forests. The study finds that machine learning techniques such as convoluted neural networks do not improve Hgb level measurement accuracy given the current sample size of the study population; we would like to refine the algorithm and test on a wider range of data as well as techniques such as neural networks and Bayesian regressions. We aim to develop quality control algorithms accounting for other common irregularities in images of fingernails that could lead to inaccurate Hgb level estimation including presence of abnormal fingernail bed pigmentation, abnormal imaging brightness, and lack of image focus, using convolution of fingernail bed images with edge detection kernels to detect edges within the fingernail beds corresponding to abrupt color changes caused by abnormal fingernail bed pigmentation which results in improved prediction accuracy.

## 2. Methodology

## 2.1 Multilinear Regression

### Data

We examine 72 of the images collected from patients enrolled in Dr. Nirmish Shah's clinic. Each patient has four images corresponding to different fingers. The images are processed such that the nail bed is captured in a bounding box while the background of the image is discarded. Colour information of each pixel is extracted from the bounded nail bed images as features.

We know that each pixel can be represented by RGB values, but the RGB colour space contains both colour information and the light information, which is different for each image since photos are taken at different times and settings. To eliminate the inconsistency caused by variation in lightning and background, we used two other colour spaces (HSV, LAB) to separate the colour information from the lightning information. We computed the mean of each value/channel across the bounded nail bed image for each of these three colour spaces (HSV, LAB, RGB) and used them as our model input.

Our response variable is blood hemoglobin concentration associated with each nail bed.

Our predictor variables are:

-   mean_H: mean value of hue (the color component / base pigment) of HSV color space.

-   mean_S: mean value of saturation (amount of color / depth of the pigment / dominance of hue) of HSV color space.

-   mean_V: mean value of value (brightness of the color) of HSV color space.

-   mean_L: mean value of lightness from black to white on a scale of 0 - 100 of LAB color space.

-   mean_A: mean value of representation of greenness to redness on a scale of -128 to +127 of LAB color space.

-   mean_B: mean value of representation of blueness to yellowness on a scale of -128 to +127 of LAB color space.

-   mean_Prop_R: mean value of redness of RGB color space.

-   mean_Prop_G: mean value of greenness of RGB color space.

-   mean_Prop_B: mean value of blueness of RGB color space.

Since the dataset is private information of the patients enrolled is Dr. Shah' clinic, we would not include the details of the data in this analysis.

```{r, echo=FALSE}
# function to calculate model fit statistics
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}
```

```{r, warning=F, message=F, echo=F}
#| label: load-pkgs
#| message: false
 
library(tidyverse)
library(tidymodels)
library(knitr)
library(glmnet)
```

## Glimpse of Data

```{r, echo=F}
#| label: load-data
#| message: false

nail_data <- read_csv("nail_data.csv") |> 
  rename(concentration = `Concentration (g/dL)`)

glimpse(nail_data)
```


## LASSO Model

### Cross Validation for Hyperparameter Tuning

```{r, echo=F}
#| label: initial-split
set.seed(123)
nail_lasso<-nail_data%>%
  select(-Image_URL,-xmin,-xmax,-ymin,-ymax)
x <- model.matrix(concentration~.,nail_lasso)[,-1]
y <- nail_lasso$concentration
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

```{r, echo=F}
grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
# Find the best lambda using cross-validation
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

lasso.model <- glmnet(x, y, alpha = 1,lambda = bestlam)
coef(lasso.model)
```

## Predictions

```{r}
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2)
```

## Discussion of Model Output

We have derived the following linear model:

\begin{align}\hat Hgb & = 1356.390+0.004 \times Mean\_H+31.719 \times Mean\_S \\& +16.880 \times Mean\_V -0.126 \times Mean\_L-0.435 \times Mean\_A \\& -0.257 \times Mean\_B-1306.425 \times Mean\_Prop\_R \\& -1435.883 \times Mean\_Prop\_G-1319.198 \times Mean\_Prop\_B \end{align}

We notice that while variables `Mean_L`, `Mean_A`, `Mean_B`, `Mean_Prop_R`, `Mean_Prop_G`, and `Mean_Prop_B` are negatively associated with the response variable `Hgb` concentration, `Mean_H`, `Mean_S` and `Mean_V` are positively associated with the response variable. This means as mean value of hue, or mean value of saturation, or mean value of brightness of HSV color space increases, the hemoglobin concentration level also tends to increase. On the other hand, as the mean value of brightness, redness, greenness or blueness of RGB color space increases, the hemoglobin concentration level tends to decrease.

We found that out of all the predictors, `Mean_S` tends to associates most significantly with the response variable with the smallest p value. For each 1 unit increase in the mean value of saturation, Hgb concentration is expected to increase by 31.719 on average, keeping all else constant. The second most significant predictor is `Mean_A`, by which we found that for each 1 unit increase in the mean value of representation of greenness to redness, Hgb concentration is expected to decrease by 0.435 on average, keeping all else constant.

## 2.2 Principle Components Analysis (PCA)

### Data

In the first part of our research, we used the TBND_V2 (Transient Biometrics Nails Dataset) on Kaggle \[1\], which contains unlabeled nail bed images [@kaggle].

### Experiment on TBND_V2 Dataset

We initially experimented with the TBND_V2 dataset found on Kaggle, since there is not enough data from the patients. Given that the data is unlabelled, we tried unsupervised clustering methods on the data, hoping to gain some insights on classification of nail bed images.

To get features, we used VGG16, a convolutional neural network (CNN) in our model. It extracts features from the input images, turning each image into feature vectors (4096 by 1). We removed the final (prediction) layer from the neural network manually, and the new output layer is a fully-connected layer with 4,096 individual nodes. We do this by specifying the "outputs" argument when initialising the model. We therefore get input of our model by using the neural net VGG16 as a feature extractor for the image data.

We then performed a principal component analysis (PCA) on the feature vectors to reduce the dimension of the feature space. For each of the 93 image samples, we now have a corresponding 1 by 4096 feature vector. This means that our model needs to process a 93 by 4096 matrix. To reduce the computational and complexity cost of processing high-dimensional data, we performed principal component analysis (PCA) on the matrix for dimension reduction. We set the parameter to 50 to obtain the top 50 principal components of the feature vector. The principal components are by default sorted in descending order. This means that the first principal component will be able to explain the most variability in the feature vector. It's a linear combination of the feature variables, and its direction captures the most variability. Thus, PCA helps us to reduce the dimension of the features from 4096 to 50 while preserving as much information in the original data as possible.

After getting features, we used Kmean clustering, an unsupervised algorithm that is commonly used in exploratory data analysis, to perform the clustering. The Kmean clustering works as follows: Initialise the centre of each k cluster by shuffling the dataset and randomly selecting K data points without replacement. Iterate the following steps until the assignment of data points to clusters is no longer changing: Compute the sum of the Euclidean distance squared between data points and all centres. Assign each data point to the closest cluster; each cluster is represented by its unique centre point. Compute the cluster's centroid by taking the average of all data points assigned to that cluster.

In our model, we set the hyperparameter k to be 5 and clustered all samples into 5 categories. Based on the features we extracted, each cluster will contain images that are visually similar.

### PCA Results

Based on the documentation, the explained_variance_ratio\_ function returns the percentage of variability explained by each of the selected components. Running this function gives us the amount of variability that is explained by all the PCs (0.1015 is explained by the first PC, 0.0894 by the second, and 0.0781 by the third etc.)

![PCA variance ratios](ratios.png){alt="PCA variance ratios"}

Then we generated a bar chart to represent the variability explained by different principal components, as well as the cumulative step plot to represent the variability explained by the first most important components.

![PCA cumulative step plot](PCA.png){alt="PCA cumulative step plot"}

The neural net in the model functions as a feature extractor for the image data, which is the input of our model. To be more specific, we used VGG16, a convolutional neural network (CNN) in our model. It extracts features from the input images, turning each image into feature vectors (4096 by 1). We removed the final (prediction) layer from the neural network manually, and the new output layer is a fully-connected layer with 4,096 individual nodes. We do this by specifying the "outputs" argument when initialising the model.

The PCA reduces dimensions of our feature vectors. For each of the 93 image samples, we now have a corresponding 1 by 4096 feature vector. This means that our model needs to process a 93 by 4096 matrix. To reduce the computational and complexity cost of processing high-dimensional data, we performed principal component analysis (PCA) on the matrix for dimension reduction. We set the parameter to 50 to obtain the top 50 principal components of the feature vector. The principal components are by default sorted in descending order. This means that the first principal component will be able to explain the most variability in the feature vector. It's a linear combination of the feature variables, and its direction captures the most variability. Thus, PCA helps us to reduce the dimension of the features from 4096 to 50 while preserving as much information in the original data as possible.

In our model, we set the hyperparameter k to be 5 and clustered all samples into 5 categories. Based on the features we extracted, each cluster will contain images that are visually similar.


### 3. Conclusion

In this study, we explored primarily two methods in predicting hemoglobin concentration level based on parameters derived from patients' nail beds images: (1) multilinear regression, (2) principle components analysis. While the first model provides reasonable train and and test error, the p value of the predictors seems to be large. Our second model yields a reasonable explained variance ratio with respect to the scale of principle components. Limitations of our work includes that the data from the clinic lacks the label, therefore we are unable to use classification methods such as random forest or SVM. Our future work includes fitting Bayesian regressions on the data, exploring more machine learning methods such as random forest or support vector machines, and using shrinkage methods such as ridge regression and LASSO.


### References
