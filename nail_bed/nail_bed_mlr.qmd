---
title: "Multivariate Linear Regression"
subtitle: "Nail Bed Images"
date: "Nov 14, 2022"
format: pdf
editor: visual
bibliography: references.bib
link-citations: true
output:
  pdf_document: default
---

## 1. Introduction

Anemia is as a life-threatening disease that affects 2 billion, or approximately 1 in 3 people world wide [@GTechdissertation]. Patients suffering from chronic anemia require frequent monitoring of indicators such as the Hgb concentration in blood to track the progression of their disease. Despite high prevalence, however, the current diagnostic process requires blood tests which causes discomfort and trauma in patients, in addition to incurring high monetary cost, especially in regions of lower socio-economic development [@monetary]. Therefore, we aim to create machine learning models that enable non-invasive inexpensive diagnosis using patients' nail bed images to predict their Hgb concentration.

Previous work focuses on predicting Hgb concentration on three regressions of interest: the fingernail beds, the conjunctiva and the palmar creases. We aim to extend the previous research from several perspectives: The previous study found no significant correlation between blue pixel intensity and gold standard measured Hgb, which we aim to investigate further using more complex machine learning algorithms such as random forests. The study finds that machine learning techniques such as convoluted neural networks do not improve Hgb level measurement accuracy given the current sample size of the study population; we would like to refine the algorithm and test on a wider range of data as well as techniques such as neural networks and Bayesian regressions. We aim to develop quality control algorithms accounting for other common irregularities in images of fingernails that could lead to inaccurate Hgb level estimation including presence of abnormal fingernail bed pigmentation, abnormal imaging brightness, and lack of image focus, using convolution of fingernail bed images with edge detection kernels to detect edges within the fingernail beds corresponding to abrupt color changes caused by abnormal fingernail bed pigmentation which results in improved prediction accuracy.

## 2. Methodology

## 2.1 Multilinear Regression

### Data

We examine 72 of the images collected from patients enrolled in Dr. Nirmish Shah's clinic. Each patient has four images corresponding to different fingers. The images are processed such that the nail bed is captured in a bounding box while the background of the image is discarded. Colour information of each pixel is extracted from the bounded nail bed images as features.

We know that each pixel can be represented by RGB values, but the RGB colour space contains both colour information and the light information, which is different for each image since photos are taken at different times and settings. To eliminate the inconsistency caused by variation in lightning and background, we used two other colour spaces (HSV, LAB) to separate the colour information from the lightning information. We computed the mean of each value/channel across the bounded nail bed image for each of these three colour spaces (HSV, LAB, RGB) and used them as our model input. Our response variable is blood hemoglobin concentration associated with each nail bed in g/dL. Our predictor variables include:

| Variable Name            | Description                                                                                                                               |
|-----------------|-------------------------------------------------------|
| Mean value of Hue        | Average value of Hue component (the color component / base pigment) of the Hue-Saturation-Value color space                               |
| Mean value of Saturation | Average value of Saturation component (amount of color / depth of the pigment / dominance of hue) of the Hue-Saturation-Value color space |
| Mean value of Value      | Average value of Value component (brightness of the color) of the Hue-Saturation-Value color space                                        |
| Mean value of Lightness  | Average value of Lightness component from black to white on a scale of 0 - 100 of the LAB color space                                     |
| Mean Value of A          | Average value of representation of greenness to redness on a scale of -128 to +127 of LAB color space                                     |
| Mean Value of B          | Average value of representation of blueness to yellowness on a scale of -128 to +127 of LAB color space                                   |
| Mean Value of R          | Average value of redness of Red-Green-Blue color space                                                                                    |
| Mean Value of G          | Average value of greenness of Red-Green-Blue color space                                                                                  |
| Mean Value of B          | Average value of blueness of Red-Green-Blue color space                                                                                   |

Since the dataset is private information of the patients enrolled is Dr. Shah' clinic, we would not include the details of the data in this analysis. We provide a glimpse of the data below.

```{r, warning=F, message=F, echo=F}
#| label: load-pkgs
#| message: false
 
library(tidyverse)
library(tidymodels)
library(knitr)
library(glmnet)
```

```{r, echo=F}
#| label: load-data
#| message: false

nail_data <- read_csv("nail_data.csv") |> 
  rename(concentration = `Concentration (g/dL)`)

glimpse(nail_data)
```

```{r EDA, message=F, echo=F, warning=F, fig.cap='Distribution of response variable'}

ggplot(nail_data, aes(x=concentration)) + 
  geom_histogram(bins=20) + 
  labs(x = 'Concentration of Hemoglobin in g/dL', 
       y = 'Count', 
       title = 'Distribution of hemoglobin concentration')

nail_data |> 
  select(concentration) |> 
  summary(concentration) |> 
  kable(digits=3)
```

Figure 1 and the corresponding summary statistics show that the Hemoglobin concentration in g/dL ranges from 9.20 g/dL to 13.30 g/dL, with a mean of 10.43 g/dL and median of 9.90 g/dL, which better captures the center of the distribution since it is right-skewed.

## LASSO Model

### Cross Validation for Hyperparameter Tuning

```{r, echo=F}
#| label: initial-split
set.seed(123)
nail_lasso<-nail_data%>%
  select(-Image_URL,-xmin,-xmax,-ymin,-ymax)
x <- model.matrix(concentration~.,nail_lasso)[,-1]
y <- nail_lasso$concentration
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

```{r, echo=F}
grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
# Find the best lambda using cross-validation
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

lasso.model <- glmnet(x, y, alpha = 1,lambda = bestlam)
coef(lasso.model)
```

## Predictions

```{r}
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2)
```

## Discussion of Model Output

We have derived the following linear model:

```{=tex}
\begin{align}\hat Hgb & = 1356.390+0.004 \times Mean\_H+31.719 \times Mean\_S \\& +16.880 \times Mean\_V -0.126 \times Mean\_L-0.435 \times Mean\_A \\& -0.257 \times Mean\_B-1306.425 \times Mean\_Prop\_R \\& -1435.883 \times Mean\_Prop\_G-1319.198 \times Mean\_Prop\_B \end{align}
```
We notice that while variables `Mean_L`, `Mean_A`, `Mean_B`, `Mean_Prop_R`, `Mean_Prop_G`, and `Mean_Prop_B` are negatively associated with the response variable `Hgb` concentration, `Mean_H`, `Mean_S` and `Mean_V` are positively associated with the response variable. This means as mean value of hue, or mean value of saturation, or mean value of brightness of HSV color space increases, the hemoglobin concentration level also tends to increase. On the other hand, as the mean value of brightness, redness, greenness or blueness of RGB color space increases, the hemoglobin concentration level tends to decrease.

We found that out of all the predictors, `Mean_S` tends to associates most significantly with the response variable with the smallest p value. For each 1 unit increase in the mean value of saturation, Hgb concentration is expected to increase by 31.719 on average, keeping all else constant. The second most significant predictor is `Mean_A`, by which we found that for each 1 unit increase in the mean value of representation of greenness to redness, Hgb concentration is expected to decrease by 0.435 on average, keeping all else constant.

## 3. Conclusion

In this study, we explored how multilinear regression can be used to predict hemoglobin concentration level based on features derived from patients' nail beds images. While the LASSO model provides reasonable train and and test error, the p value of the predictors seems to be large.

Our second model yields a reasonable explained variance ratio with respect to the scale of principle components. Limitations of our work includes that the data from the clinic lacks the label, therefore we are unable to use classification methods such as random forest or SVM. Our future work includes fitting Bayesian regressions on the data, exploring more machine learning methods such as random forest or support vector machines, and using shrinkage methods such as ridge regression and LASSO.

In the future, we would like to compare the utility of adding features such as those extracted by a convolutional neural network (CNN). To begin a comparison, we ask: - How much variability is explained by the principal components of CNN feature set? It would be interesting to compare these principal components to the principal components of the features outlined in section 2.1 but will required additional data processing. See appendix for a detailed description of our preliminary exploration.

## Appendix

Prior to having obtained data used in main analysis, we explored the TBND_V2 (Transient Biometrics Nails Dataset) on Kaggle, which contains unlabeled nail bed images [@kaggle].

With the unlabelled data, we try unsupervised clustering methods such as Kmeans clustering to gain some insights on classifying nail bed images. We use VGG16, a convolutional neural network (CNN) to exxtract features from the input images, turning each image into a feature vector with 4096 entries. We remove the final (prediction) layer from the neural network manually, and the new output layer is a fully-connected layer with 4,096 individual nodes. We do this by specifying the "outputs" argument when initialising the model. We therefore get input of our model by using the neural net VGG16 as a feature extractor for the image data.

We then perform a principal component analysis (PCA) on the feature vectors to reduce the dimension of the feature space. For each of the 93 image samples, we now have a corresponding 1 by 4096 feature vector. This means that our model needs to process a 93 by 4096 matrix. To reduce the computational and complexity cost of processing high-dimensional data, we perform a principal component analysis (PCA) on the matrix for dimension reduction. We set the parameter to 50 to obtain the top 50 principal components of the feature vector. The principal components are by default sorted in descending order. This means that the first principal component will be able to explain the most variability in the feature vector. It's a linear combination of the feature variables and its direction captures the most variability. Thus, PCA helps us to reduce the dimension of the features from 4096 to 50 while preserving as much information in the original data as possible.

### PCA Results

Based on the documentation, the explained_variance_ratio\_ function returns the percentage of variability explained by each of the selected components. Running this function gives us the amount of variability that is explained by all the PCs (0.1015 is explained by the first PC, 0.0894 by the second, and 0.0781 by the third etc.) The table below shows the top 10 principal components:

| PC1     | PC2     | PC3     | PC4     | PC5     | PC6     | PC7     | PC8     | PC9     | PC10    |
|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|
| 0.10152 | 0.08947 | 0.07806 | 0.06449 | 0.05609 | 0.04782 | 0.04122 | 0.03169 | 0.03162 | 0.02549 |

We report a bar chart to represent the variability explained by different principal components, as well as the cumulative step plot to represent the variability explained by the first most important components.

![PCA cumulative step plot](PCA.png){alt="PCA cumulative step plot"}

The neural net in the model functions as a feature extractor for the image data, which is the input of our model. To be more specific, we used VGG16, a convolutional neural network (CNN) in our model. It extracts features from the input images, turning each image into feature vectors (4096 by 1). We removed the final (prediction) layer from the neural network manually, and the new output layer is a fully-connected layer with 4,096 individual nodes. We do this by specifying the "outputs" argument when initialising the model.

## References
