---
title: "Multivariate Linear Regression"
subtitle: "Nail Bed Images"
date: "Nov 14, 2022"
format: pdf
editor: visual
bibliography: references.bib
link-citations: true
output:
  pdf_document: default
---

## 1. Introduction

Anemia is as a life-threatening disease that affects 2 billion, or approximately 1 in 3 people world wide [@GTechdissertation]. In 2019, this long lasting disease accounted for 50.3 million YLD (years lived with disabilities) [@monetary]. Patients suffering from chronic anemia require frequent monitoring of indicators such as the Hgb concentration in blood to track the progression of their disease. Despite high prevalence, however, the current diagnostic process requires blood tests which causes discomfort and trauma in patients, in addition to incurring high monetary cost, especially in regions of lower socio-economic development [@monetary]. According to studies in cost-effectiveness of anemia screening, total costs per life saved in targeted screening amounted to \$3575, while treating anemia can cost between \$18 and \$500 per month depending on the type of anemia and necessary treatment [@cost]. Therefore, we aim to create machine learning models that enable non-invasive inexpensive diagnosis using patients' nail bed images to predict their Hgb concentration.

Previous work focuses on predicting Hgb concentration on three regressions of interest: the fingernail beds, the conjunctiva and the palmar creases. We aim to extend the previous research by developing a multilinear LASSO regression model, using features extracted from the nail bed images as predictors and cross validation for hyperparameter tuning. We also compared our results with previous work to examine the correlation between the features and the hemoglobin concentration levels.

## 2. Methodology: LASSO Multilinear Model

### Data

We examine 72 of the images collected from patients enrolled in Dr. Nirmish Shah's clinic. Each patient has four images corresponding to different fingers. The images are processed such that the nail bed is captured in a bounding box while the background of the image is discarded. Colour information of each pixel is extracted from the bounded nail bed images as features.

We know that each pixel can be represented by RGB values, but the RGB colour space contains both colour information and the light information, which is different for each image since photos are taken at different times and settings. To eliminate the inconsistency caused by variation in lightning and background, we used two other colour spaces (HSV, LAB) to separate the colour information from the lightning information. We computed the mean of each value/channel across the bounded nail bed image for each of these three colour spaces (HSV, LAB, RGB) and used them as our model input. Our response variable is blood hemoglobin concentration associated with each nail bed in g/dL. Our predictor variables include:

| Variable Name            | Description                                                                                                                               |
|------------------|------------------------------------------------------|
| Mean value of Hue        | Average value of Hue component (the color component / base pigment) of the Hue-Saturation-Value color space                               |
| Mean value of Saturation | Average value of Saturation component (amount of color / depth of the pigment / dominance of hue) of the Hue-Saturation-Value color space |
| Mean value of Value      | Average value of Value component (brightness of the color) of the Hue-Saturation-Value color space                                        |
| Mean value of Lightness  | Average value of Lightness component from black to white on a scale of 0 - 100 of the LAB color space                                     |
| Mean Value of A          | Average value of representation of greenness to redness on a scale of -128 to +127 of LAB color space                                     |
| Mean Value of B          | Average value of representation of blueness to yellowness on a scale of -128 to +127 of LAB color space                                   |
| Mean Value of R          | Average value of redness of Red-Green-Blue color space                                                                                    |
| Mean Value of G          | Average value of greenness of Red-Green-Blue color space                                                                                  |
| Mean Value of B          | Average value of blueness of Red-Green-Blue color space                                                                                   |

Since the dataset is private information of the patients enrolled is Dr. Shah' clinic, we would not include the details of the data in this analysis. We provide a glimpse of the data below.

```{r, warning=F, message=F, echo=F}
#| label: load-pkgs
#| message: false
 
library(tidyverse)
library(tidymodels)
library(knitr)
library(glmnet)
```

```{r, echo=F}
#| label: load-data
#| message: false

nail_data <- read_csv("nail_data.csv") |> 
  rename(concentration = `Concentration (g/dL)`)

glimpse(nail_data)
```

### Exploratory Data Analysis

```{r EDA, message=F, echo=F, warning=F, fig.cap='Distribution of response variable'}

ggplot(nail_data, aes(x=concentration)) + 
  geom_histogram(bins=20) + 
  labs(x = 'Concentration of Hemoglobin in g/dL', 
       y = 'Count', 
       title = 'Distribution of hemoglobin concentration')

nail_data |> 
  select(concentration) |> 
  summary(concentration) |> 
  kable(digits=3)
```

Figure 1 and the corresponding summary statistics show that the Hemoglobin concentration in g/dL ranges from 9.20 g/dL to 13.30 g/dL, with a mean of 10.43 g/dL and median of 9.90 g/dL, which better captures the center of the distribution since it is right-skewed.

### Cross Validation for Hyperparameter Tuning

In order to perform variable selection to gauge insight into the predictors which have significantly associations with hemoglobin concentrations, we use the LASSO which uses $L_1$ norm penalty, shrinking the coefficient estimates of insignificant predictors towards zero.

We use cross validation to tune the hyperparamter $\lambda$ and visualize the shrinkage of the coefficients:

```{r, echo=F}
set.seed(123)
nail_lasso<-nail_data%>%
  select(-Image_URL,-xmin,-xmax,-ymin,-ymax)
x <- model.matrix(concentration~.,nail_lasso)[,-1]
y <- nail_lasso$concentration
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

We observe that the best $\lambda$ which results in the smallest MSE is 0.0584

```{r, echo=F}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
# Find the best lambda using cross-validation
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

lasso.model <- glmnet(x, y, alpha = 1,lambda = bestlam)
df<-as.data.frame(as.matrix(coef(lasso.model)))
df%>%filter(s0!=0)%>%rename("coefficient"=s0)%>%kable(digits=4)
```

As a result, we derive a sparse model which only involves a subset of the features extracted from the nailbed images.

## Predictions

```{r}
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2)
```

Performing predictions on the test data set, we derive a test MSE of 14.019.

## Discussion of Model Output

We have derived the following linear model:

$$\hat Hgb = 15.4670+0.0045 \times Mean\_H+11.6495 \times Mean\_S +0.0124 \times Mean\_L-30.2895 \times Mean\_Prop\_G$$

We notice that while variables `Mean_H`, `Mean_S` and `Mean_L` are positively associated with the response variable `Hgb` concentration, `Mean\_Prop\_G` is negatively associated with the response variable. This means as mean value of hue, mean value of saturation, or mean value of value (brightness of the color) of HSV color space increases, the hemoglobin concentration level also tends to increase. On the other hand, as the mean value of greenness of RGB color space increases, the hemoglobin concentration level tends to decrease.

We find that out of all the predictors, `Mean\_Prop\_G` tends to associates most significantly with the response variable with the coefficient with the largest absolute magnitude. For each 1 unit increase in the mean value of greenness of RGB color space, Hgb concentration is expected to decrease by 30.2895 on average, keeping all else constant. 

## 3. Conclusion

In this study, we explored how multilinear regression can be used to predict hemoglobin concentration level based on features derived from patients' nail beds images. The LASSO model selects 4 predictors (mean value of Hue, mean value of Saturation, mean value of Lightness, and mean value of representation for greenness) among the 9 predictors. Previous research have found that representation of blueness in RGB color space is not a significant predictor [@GTechdissertation] for blood hemoglobin levels. This notion is consistent with our result that mean value of representation of blueness is included in the final model. Among the 4 predictors chosen by LASSO model, mean value of Saturation has the strongest positive relationship with the response, and mean value of representation for greenness has a negative relationship with the response. In the future, we would like to explore more on these two predictors.

Limitations of our work includes that the data from the clinic lacks the label, therefore we are unable to use classification methods such as random forest or SVM. Our future work includes fitting Bayesian regressions on the data, exploring more machine learning methods such as random forest or support vector machines, and using other shrinkage methods such as ridge regression. We would also like to compare the utility of adding features such as those extracted by a convolutional neural network (CNN). To begin a comparison, we ask: - How much variability is explained by the principal components of CNN feature set? It would be interesting to compare these principal components to the principal components of the features outlined in section 2.1 but will required additional data processing. See appendix for a detailed description of our preliminary exploration.

## Appendix

Prior to having obtained data used in main analysis, we explored the TBND_V2 (Transient Biometrics Nails Dataset) on Kaggle, which contains unlabeled nail bed images [@kaggle].

With the unlabelled data, we try unsupervised clustering methods such as Kmeans clustering to gain some insights on classifying nail bed images. We use VGG16, a convolutional neural network (CNN) to exxtract features from the input images, turning each image into a feature vector with 4096 entries. We remove the final (prediction) layer from the neural network manually, and the new output layer is a fully-connected layer with 4,096 individual nodes. We do this by specifying the "outputs" argument when initialising the model. We therefore get input of our model by using the neural net VGG16 as a feature extractor for the image data.

We then perform a principal component analysis (PCA) on the feature vectors to reduce the dimension of the feature space. For each of the 93 image samples, we now have a corresponding 1 by 4096 feature vector. This means that our model needs to process a 93 by 4096 matrix. To reduce the computational and complexity cost of processing high-dimensional data, we perform a principal component analysis (PCA) on the matrix for dimension reduction. We set the parameter to 50 to obtain the top 50 principal components of the feature vector. The principal components are by default sorted in descending order. This means that the first principal component will be able to explain the most variability in the feature vector. It's a linear combination of the feature variables and its direction captures the most variability. Thus, PCA helps us to reduce the dimension of the features from 4096 to 50 while preserving as much information in the original data as possible.

### PCA Results

Based on the documentation, the explained_variance_ratio\_ function returns the percentage of variability explained by each of the selected components. Running this function gives us the amount of variability that is explained by all the PCs (0.1015 is explained by the first PC, 0.0894 by the second, and 0.0781 by the third etc.) The table below shows the top 10 principal components:

| PC1     | PC2     | PC3     | PC4     | PC5     | PC6     | PC7     | PC8     | PC9     | PC10    |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| 0.10152 | 0.08947 | 0.07806 | 0.06449 | 0.05609 | 0.04782 | 0.04122 | 0.03169 | 0.03162 | 0.02549 |

We report a bar chart to represent the variability explained by different principal components, as well as the cumulative step plot to represent the variability explained by the first most important components.

![PCA cumulative step plot](PCA.png){alt="PCA cumulative step plot"}

The neural net in the model functions as a feature extractor for the image data, which is the input of our model. To be more specific, we used VGG16, a convolutional neural network (CNN) in our model. It extracts features from the input images, turning each image into feature vectors (4096 by 1). We removed the final (prediction) layer from the neural network manually, and the new output layer is a fully-connected layer with 4,096 individual nodes. We do this by specifying the "outputs" argument when initialising the model.

## References
