---
title: "Multivariate Linear Regression"
subtitle: "Nail Bed Images"
date: "Nov 14, 2022"
format: pdf
editor: visual
bibliography: references.bib
link-citations: true
---

## Background & Literature Review

Anemia is as a life-threatening disease that affects 2 billion, or approximately 1 in 3 people world wide [@GTechdissertation].

patients suffereing from chronic anemia require frequent monitoring of indicators such as the Hgb levels to track the progression of their disease. Despite this high prevalence, however, the current diagnosis process requires blood tests which causes discomfort and trauma in patients, in addition to incurring high monetary costs. Therefore, our research aims to create machine learning models that enable non-invasive inexpensive diagnosis using patients' nail bed images to predict their Hgb level which will prove particularly crucial for patients in underresourced communities, possibly using random forest algorithms and neural networks.

To this end, we based our research on prior studies that focused on predicting Hgb levels on three regreions of interest: the fingernail beds, the conjunctiva and the palmar creases. With the Hgb estimation algorithm developed via a custom generated MATLAB function which correlated the 3 color RGB channels with the gold-standard measured Hgb levels, the predicted Hgb levels strongly correlate with Hgb levels determined by the clinical hemotology analyzer, with a correlation of determination of 0.995, indicating that this technique can be used to accurately measure a patient's hemoglobin level. We aim to extend the previous research from several perspectives: (1) The previous study did not find significant correlation between blue pixel intensity and gold standard measured Hgb, which we aim to investigate further using more complex machine learning algorithms such as random forests. (2) The study finds that machine learning techniques do not improve Hgb level measurement accuracy given the current sample size of the study population; we would like to refine the algorithm and test on a wider range of data as well as techniques such as neural networks and Bayesian regressions. (3) We aim to develop quality control algorithms accounting for other common irregularities in images of fingernails that could lead to inaccurate Hgb level estimation including presence of abnormal fingernail bed pigmentation, abnormal imaging brightness, and lack of image focus, using convolution of fingernail bed images with edge detection kernels to detect edges within the fingernail beds corresponding to abrupt color changes caused by abnormal fingernail bed pigmentation which results in improved prediction accuracy.

## Function

```{r}
# function to calculate model fit statistics
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}
```

## Packages

```{r, warning=F, message=F}
#| label: load-pkgs
#| message: false
 
library(tidyverse)
library(tidymodels)
library(knitr)
```

## Load data

```{r}
#| label: load-data
#| message: false

nail_data <- read_csv("nail_data.csv") |> 
  rename(concentration = `Concentration (g/dL)`)

glimpse(nail_data)
```

## Split data into training and testing

Split your data into testing and training sets.

```{r}
#| label: initial-split

set.seed(123)
nail_split <- initial_split(nail_data)
nail_train <- training(nail_split)
nail_test <- testing(nail_split)
```

## Specify model

```{r}
#| label: specify-model

nail_spec <- linear_reg() |>
  set_engine("lm")

nail_spec
```

## Create recipe

```{r}
#| label: create-recipe

nail_rec <- recipe(concentration ~ ., data = nail_train) |>
  update_role(Image_URL, new_role = "id") |>
  step_rm(xmin, xmax, ymin, ymax) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

nail_rec
```

## Create workflow

```{r}
#| label: create-wflow
nail_wflow <- workflow() |>
  add_model(nail_spec) |>
  add_recipe(nail_rec)

nail_wflow
```

# Cross validation

## Create folds

Create 10-folds.

```{r}
#| label: cv-tenfold

# make 10 folds
set.seed(1)
folds <- vfold_cv(nail_train, v = 10)
folds
```

## Conduct cross validation

Conduct cross validation on the 10 folds.

```{r}
#| label: conduct-cv

set.seed(456)
# Fit model and calculate statistics for each fold
nail_fit_rs <- nail_wflow |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))
```

## Summarize assessment CV metrics

Summarize assessment metrics from your CV resamples.

```{r}
#| label: cv-summarize

collect_metrics(nail_fit_rs, summarize = FALSE) # summarize = FALSE to see individualized output; when comparing two models, set summarize = TRUE
```

## Summarize model fit CV metrics

```{r}
#| label: cv-model-fit
map_df(nail_fit_rs$.extracts, ~ .x[[1]][[1]]) |> #model stats are in .extracts column 
  summarise(mean_adj_rsq = mean(adj.r.squared), # avg_stats computed over 10 folds 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

```{r model}
nail_fit_train <- nail_wflow |>
      fit(data = nail_train)

tidy(nail_fit_train) |> 
  kable(digits=3)
```

```{r test}
nail_train_pred <- predict(nail_fit_train, nail_train) |>
        bind_cols(nail_train)

rmse_train <- rmse(nail_train_pred, truth = concentration, estimate = .pred)
   
nail_test_pred <- predict(nail_fit_train, nail_test) |>
        bind_cols(nail_test)

rmse_test <- rmse(nail_test_pred, truth = concentration, estimate = .pred)
   
model_tibble <- tibble(RMSE_train=rmse_train$.estimate, RMSE_test=rmse_test$.estimate)

model_tibble |>
        kable()
```
